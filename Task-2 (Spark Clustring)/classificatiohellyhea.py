# -*- coding: utf-8 -*-
"""ClassificatioHellYhea

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11YLAuJ_RXJWAJ_unp97Gw3vRSjnPKgwz

this library need ,  first install them
"""

!pip install hazm
!pip install python-bidi
!pip install arabic-reshaper

import pylab as pl
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle

from collections import Counter
from hazm import *

# persian transformer
from bidi.algorithm import get_display
import arabic_reshaper

# sklearn package
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

from google.colab import drive
drive.mount('/content/drive/')
path = "/content/drive/MyDrive/Colab Notebooks/data/"
# comments-forClassificationAndClutering.csv

dataset = pd.read_csv(path+ "comments-forClassificationAndClutering.csv")
dataset['split'] = np.random.randn(dataset.shape[0], 1)
msk = np.random.rand(len(dataset)) <= 0.7
train = dataset[msk]
train['body'].isnull().sum()
train['recommendation_status'].isnull().sum()
train = train[train['body'].notna()]
train = train[train['recommendation_status'].notna()]
train= train.reset_index(level=0)
# test
test = dataset[~msk]
test['body'].isnull().sum()
test['recommendation_status'].isnull().sum()
test = test[test['body'].notna()]
test = test[test['recommendation_status'].notna()]
test= test.reset_index(level=0)

print(train)

"""# read train & test data"""

def read(name = 'HAMtrain'):


    data = pd.read_csv('{}.txt'.format(name), header = None,sep='@@@@@@@@@@',engine='python',encoding='utf-8')

    # data[0] >>>>>>>>>>>>>>>>label
    # data[1] >>>>>>>>>>>>>>>>news
    # data.shape : (2580,2)

    return data

"""# plot distribution of topics in train corpus"""

# plot for persian  it's not OK  this function solve this problem
def make_farsi_text(x):
    output=[]
    for i in x:
        reshaped_text = arabic_reshaper.reshape(i)
        farsi_text    = get_display(reshaped_text)
        output.append(farsi_text)
    return output

def barplot():

    data=dataset
    dfToList = data['recommendation_status'].tolist()

    #find number of each news topic
    counts=Counter(dfToList)
    
    # as we corpus has six topic and we can visualize them by unique color

    X = np.arange(len(counts))
    pl.bar(X, counts.values(), align='center', width=0.5,color=['red','orange','lightblue','lightgreen','green'])
    pl.xticks(X,make_farsi_text(counts.keys()))
    ymax = max(counts.values()) +20
    pl.ylim(0, ymax)
    plt.savefig('topic.png')
    pl.show()

barplot()

"""# preprocess of data"""

def preprocessing(datasetName):

    data = datasetName
    
# Use Hazm for preprocess of data :
    stopwords  = stopwords_list()
    normalizer = Normalizer()
    lemmatizer = Lemmatizer()
    puncList   = [".", ";", ":", "!", "?", "/", "\\", ",", "#", "@", "$", "&", ")", "(", "\"", "\'\'"]
    stopwords.append(puncList)

    for i in range(len(data)):
        print(i)

        data['body'][i] = normalizer.normalize(data['body'][i])
        data['body'][i] = word_tokenize(data['body'][i])
        data['body'][i] = [w for w in  data['body'][i] if not w in stopwords]
        data['body'][i] = [lemmatizer.lemmatize(w) for w in  data['body'][i]]

        
        
    
    return data

"""# Feature Extraction by tf-idf"""

def model():
    data      = preprocessing(train)
    data_test = preprocessing(test)
    

    X_train   = data["body"].tolist()
    y_train   = data['recommendation_status'].tolist()
    
    del data

    X_test    = data_test["body"].tolist()
    y_test    = data_test['recommendation_status'].tolist()
    
    del data_test
    
#     conver lables of data to numbers 
    label_encoder = LabelEncoder()
    y_train       = label_encoder.fit_transform(y_train)
    y_test        = label_encoder.transform(y_test)


    countV  = CountVectorizer(min_df=2, max_features=89499 ,lowercase=False,tokenizer=lambda doc: doc)
    X_train = countV.fit_transform(X_train)
    tfidf   = TfidfTransformer()
    X_train = tfidf.fit_transform(X_train)
    
    print(X_train.shape)

    X_test  = countV.transform(X_test)
    X_test  = tfidf.transform(X_test)
    
    print(X_test.shape)
    
    

    with open(path+'X_train.pickle', 'wb') as output:
       pickle.dump(X_train, output)
    
    with open(path+'X_test.pickle', 'wb') as output:
       pickle.dump(X_test, output)
      
    with open(path+'y_train.pickle', 'wb') as output:
       pickle.dump(y_train, output)
    
    with open(path+'y_test.pickle', 'wb') as output:
       pickle.dump(y_test, output)

model()

"""# now we can try different models of classifier

first we should load preprocced data
"""

with open(path+'X_train.pickle', 'rb') as data:
   X_train = pickle.load(data)
    
with open(path+'y_train.pickle', 'rb') as data:
    y_train = pickle.load(data)
    
with open(path+'X_test.pickle', 'rb') as data:
    X_test = pickle.load(data)
    
with open(path+'y_test.pickle', 'rb') as data:
    y_test = pickle.load(data)

"""**As you can see we have 6 different model we get result and comparise them**

we use Cross-Validation for Hyperparameter tuning and use  GridSearchCV for find best parameter for each Model

# 1

---


**RandomForest Model :**
"""

# first we tune Hyperparameter and find this values best for this model
RFC = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',
            max_depth=40, max_features='sqrt', max_leaf_nodes=None,
            min_impurity_decrease=0.0,
            min_samples_leaf=1, min_samples_split=5,
            min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=1,
            oob_score=False, random_state=8, verbose=0, warm_start=False)

# train model on train data

RFC.fit(X_train, y_train)


# test model on train and test data
d = {
     'Model': 'Random Forest',
     'Training Set Accuracy': accuracy_score(y_train, RFC.predict(X_train)),
     'Test Set Accuracy': accuracy_score(y_test, RFC.predict(X_test))
}

df_models_rfc = pd.DataFrame(d, index=[0])

    

# save data

with open(path+'model-RFC.pickle', 'wb') as output:
    pickle.dump(RFC, output)
    
with open(path+'model-RFC-result.pickle', 'wb') as output:
    pickle.dump(df_models_rfc, output)
    
df_models_rfc

"""# 2

---


**SVM Model:**
"""

# first we tune Hyperparameter and find this values best for this model
SVM=svm.SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
        max_iter=-1, probability=True, random_state=8, shrinking=True, tol=0.001,
        verbose=False)

# train model on train data
SVM.fit(X_train, y_train)

# test model on train and test data
d = {
     'Model': 'SVM',
     'Training Set Accuracy': accuracy_score(y_train, SVM.predict(X_train)),
     'Test Set Accuracy': accuracy_score(y_test, SVM.predict(X_test))
}

df_models_svc = pd.DataFrame(d, index=[0])

# save data


with open(path+'model-SVM.pickle', 'wb') as output:
    pickle.dump(SVM, output)
    
with open(path+'model-SVM-result.pickle', 'wb') as output:
    pickle.dump(df_models_svc, output)

df_models_svc

"""# 3

---


**KNN model:**
"""

# first we tune Hyperparameter and find this values best for this model
KNN = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=6, p=2,
           weights='uniform')


# train model on train data

KNN.fit(X_train, y_train)

# test model on train and test data
d = {
     'Model': 'KNN',
     'Training Set Accuracy': accuracy_score(y_train, KNN.predict(X_train)),
     'Test Set Accuracy': accuracy_score(y_test, KNN.predict(X_test))
}

df_models_knnc = pd.DataFrame(d, index=[0])


# save data

with open(path+'model-KNN.pickle', 'wb') as output:
    pickle.dump(KNN, output)
    
with open(path+'model-KNN-result.pickle', 'wb') as output:
    pickle.dump(df_models_knnc, output)
    


df_models_knnc

"""# 4

---


**MultinomialNB Model:**
"""

# first we tune Hyperparameter and find this values best for this model
MNB=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)

# train model on train data

MNB.fit(X_train, y_train)

# test model on train and test data
d = {
     'Model': 'Multinomial Na√Øve Bayes',
     'Training Set Accuracy': accuracy_score(y_train, MNB.predict(X_train)),
     'Test Set Accuracy': accuracy_score(y_test, MNB.predict(X_test))
}

df_models_mnbc = pd.DataFrame(d, index=[0])

# save data

with open(path+'model-MNB.pickle', 'wb') as output:
    pickle.dump(MNB, output)
    
with open(path+'model-MNB-result.pickle', 'wb') as output:
    pickle.dump(df_models_mnbc, output)
    


df_models_mnbc

"""# 5

---


**Multinomial Logistic Regression Model:**
"""

# first we tune Hyperparameter and find this values best for this model
LR=LogisticRegression(C=1.0, class_weight='balanced', dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='multinomial', n_jobs=1, penalty='l2',
          random_state=8, solver='sag', tol=0.0001, verbose=0,
          warm_start=False)

# train model on train data
LR.fit(X_train, y_train)

# test model on train and test data
d = {
     'Model': 'Logistic Regression',
     'Training Set Accuracy': accuracy_score(y_train, LR.predict(X_train)),
     'Test Set Accuracy': accuracy_score(y_test, LR.predict(X_test))
}


df_models_lrc = pd.DataFrame(d, index=[0])


# save data


with open(path+'model-LR.pickle', 'wb') as output:
    pickle.dump(LR, output)
    
with open(path+'model-LR-result.pickle', 'wb') as output:
    pickle.dump(df_models_lrc, output)
    


df_models_lrc

"""# 6
---


**Gradient Boosting Machine Model:**
"""

# first we tune Hyperparameter and find this values best for this model
GB=GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=15,
              max_features='sqrt', max_leaf_nodes=None,
              min_impurity_decrease=0.0,
              min_samples_leaf=2, min_samples_split=50,
              min_weight_fraction_leaf=0.0, n_estimators=800, random_state=8, subsample=1.0, verbose=0,
              warm_start=False)

# train model on train data
GB.fit(X_train, y_train)

# test model on train and test data
d = {
     'Model': 'Gradient Boosting',
     'Training Set Accuracy': accuracy_score(y_train, GB.predict(X_train)),
     'Test Set Accuracy': accuracy_score(y_test, GB.predict(X_test))
}

df_models_gbc = pd.DataFrame(d, index=[0])


# save data

with open(path+'model-GB.pickle', 'wb') as output:
    pickle.dump(GB, output)
    
with open(path+'model-GB-result.pickle', 'wb') as output:
    pickle.dump(df_models_gbc, output)
    


df_models_gbc

"""# Best Model Selection"""

# load data
list_pickles = [
    "model-GB-result.pickle",
    "model-KNN-result.pickle",
    "model-LR-result.pickle",
    "model-MNB-result.pickle",
    "model-RFC-result.pickle",
    "model-SVM-result.pickle"
]

df_summary = pd.DataFrame()

for pickle_ in list_pickles:
    
    with open(path+pickle_, 'rb') as data:
        df = pickle.load(data)

    df_summary = df_summary.append(df)

df_summary = df_summary.reset_index().drop('index', axis=1)


df_summary.sort_values('Test Set Accuracy', ascending=False)

"""As I saw above *Logistic Regression* has best Test Set Accuracy 
And below we will see the results for this model

# Logistic Regression Analyze
"""

with open(path + 'model-LR.pickle', 'rb') as data:
   classifier = pickle.load(data)
    
y_pred = classifier.predict(X_test)
    
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

# predict test set
cm = confusion_matrix(y_test, y_pred)

# plot result for logistic regression

fig, ax = plt.subplots()
counts=["recommended","no_idea","not_recommended"]
dfObj = pd.DataFrame(cm, columns = make_farsi_text(counts), index=make_farsi_text(counts))
import seaborn as sns; sns.set(rc={'figure.figsize':(11.7,8.27)})

ax=sns.heatmap(dfObj, annot=True,cmap="YlGnBu",fmt='g',linewidths=0.5)
ax.set(xlabel='Predicted label', ylabel='True label')
pl.savefig('ConfMatrix.png')
pl.show()

"""# GB Analyze"""

with open(path + 'model-GB-result.pickle', 'rb') as data:
   classifier = pickle.load(data)
    
y_pred = classifier.predict(X_test)
    
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

# predict test set
cm = confusion_matrix(y_test, y_pred)

# plot result for logistic regression

fig, ax = plt.subplots()
counts=["recommended","no_idea","not_recommended"]
dfObj = pd.DataFrame(cm, columns = make_farsi_text(counts), index=make_farsi_text(counts))
import seaborn as sns; sns.set(rc={'figure.figsize':(11.7,8.27)})

ax=sns.heatmap(dfObj, annot=True,cmap="YlGnBu",fmt='g',linewidths=0.5)
ax.set(xlabel='Predicted label', ylabel='True label')
pl.savefig('ConfMatrix.png')
pl.show()

"""# Dimensionality Reduction Plots
We'll perform a dimensionality reduction technique to plot the observations in 2 dimensions.


"""

xt = pd.DataFrame(X_train.toarray())

features = xt
labels = y_train

print(features.shape)
print(labels.shape)

def plot_dim_red(model, features, labels, n_components=2):
    
    # Creation of the model
    if (model == 'PCA'):
        mod = PCA(n_components=n_components)
        title = "PCA decomposition"  # for the plot
        
    elif (model == 'TSNE'):
        mod = TSNE(n_components=2)
        title = "t-SNE decomposition" 

    else:
        return "Error"
    
    # Fit and transform the features
    principal_components = mod.fit_transform(features)
    
    # Put them into a dataframe
    df_features = pd.DataFrame(data=principal_components,
                     columns=['PC1', 'PC2'])
    
    # Now we have to paste each row's label and its meaning
    # Convert labels array to df
    df_labels = pd.DataFrame(data=labels,
                             columns=['label'])
    
    df_full = pd.concat([df_features, df_labels], axis=1)
    df_full['label'] = df_full['label'].astype(str)

    # Get labels name
    counts=["recommended","no_idea","not_recommended"]

    category=make_farsi_text(counts)
    category_names = {
        "0": category[0],
        "1": category[1],
        "2": category[2],
    }

    # And map labels
    df_full['label_name'] = df_full['label']
    df_full = df_full.replace({'label_name':category_names})

    # Plot
    plt.figure(figsize=(10,10))
    sns.scatterplot(x='PC1',
                    y='PC2',
                    hue="label_name", 
                    data=df_full,
                    palette=["red", "pink", "royalblue", "greenyellow", "lightseagreen","blue"],
                    alpha=.7).set_title(title);
    plt.savefig('PCA.png')

"""t-SNE: the t-distributed Stochastic Neighbour Embedding is a probabilistic technique particularly well suited for the visualization of high-dimensional datasets. It minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding."""

plot_dim_red("TSNE", 
             features=features, 
             labels=labels,
             n_components=2)